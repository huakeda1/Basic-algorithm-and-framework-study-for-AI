{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "def cut_sentence(data):\n",
    "    new_data=data.apply(lambda x:' '.join(jieba.lcut(x)))\n",
    "    return new_data\n",
    "def load_stopwords(stop_words_dir):\n",
    "    stopwords=[]\n",
    "    with open(stop_words_dir,'r',encoding='utf-8') as f:\n",
    "        for index,line in enumerate(tqdm(f.readlines())):\n",
    "            if not line.strip():continue\n",
    "            stopwords.append(line.strip())\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-19 02:27:51,811 - __main__ - INFO - The logger is successfully built for the model\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "logger=logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "data_path = \"./home/aistudio/data/sentiment\"\n",
    "log_path=os.path.join(data_path,'log.txt')\n",
    "# 日志记录到文件\n",
    "handler=logging.FileHandler(log_path)\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter=logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "# 日志打印到控制台\n",
    "console=logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "console.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(handler)\n",
    "logger.addHandler(console)\n",
    "logger.info('The logger is successfully built for the model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 768/768 [00:00<00:00, 1029145.52it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./home/aistudio/data/sentiment\"\n",
    "train_df=pd.read_csv(os.path.join(data_path,'sentiment.train.data'),sep='\\t',names=[\"text\",\"label\"])\n",
    "valid_df=pd.read_csv(os.path.join(data_path,'sentiment.valid.data'),sep='\\t',names=[\"text\",\"label\"])\n",
    "test_df=pd.read_csv(os.path.join(data_path,'sentiment.test.data'),sep='\\t',names=[\"text\",\"label\"])\n",
    "\n",
    "X_train=train_df['text']\n",
    "y_train=train_df['label']\n",
    "\n",
    "X_valid=valid_df['text']\n",
    "y_valid=valid_df['label']\n",
    "\n",
    "X_test=test_df['text']\n",
    "y_test=test_df['label']\n",
    "\n",
    "stop_words_dir=os.path.join(data_path,'stopwords.txt')\n",
    "stopwords=load_stopwords(stop_words_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/stu_18701958249/.local/lib/python3.7/site-packages/jieba/__init__.py\", line 154, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmptcrs4zci' -> '/tmp/jieba.cache'\n",
      "Loading model cost 0.973 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①Ａ', '①Ｂ', '①Ｃ', '①Ｄ', '①Ｅ', '①ａ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②Ｂ', '②Ｇ', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③Ｆ', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16839, 5000)\n",
      "(2111, 5000)\n",
      "(2111, 5000)\n"
     ]
    }
   ],
   "source": [
    "# get tfidf feature\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer(stop_words=stopwords,max_features=5000,lowercase=False,sublinear_tf=True,max_df=0.8)\n",
    "tfidf_vectorizer.fit(cut_sentence(X_train))\n",
    "X_train_tfidf=tfidf_vectorizer.transform(cut_sentence(X_train))\n",
    "X_valid_tfidf=tfidf_vectorizer.transform(cut_sentence(X_valid))\n",
    "X_test_tfidf=tfidf_vectorizer.transform(cut_sentence(X_test))\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_valid_tfidf.shape)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16839, 3000)\n",
      "(2111, 3000)\n",
      "(2111, 3000)\n"
     ]
    }
   ],
   "source": [
    "# feature selection\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "selector=SelectKBest(chi2,k=3000)\n",
    "X_train_tfidf_chi=selector.fit_transform(X_train_tfidf,y_train)\n",
    "X_valid_tfidf_chi=selector.transform(X_valid_tfidf)\n",
    "X_test_tfidf_chi=selector.transform(X_test_tfidf)\n",
    "print(X_train_tfidf_chi.shape)\n",
    "print(X_valid_tfidf_chi.shape)\n",
    "print(X_test_tfidf_chi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8578872572240644\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86      1059\n",
      "           1       0.86      0.85      0.86      1052\n",
      "\n",
      "    accuracy                           0.86      2111\n",
      "   macro avg       0.86      0.86      0.86      2111\n",
      "weighted avg       0.86      0.86      0.86      2111\n",
      "\n",
      "[[916 143]\n",
      " [157 895]]\n",
      "0.8531501657981999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85      1059\n",
      "           1       0.85      0.86      0.85      1052\n",
      "\n",
      "    accuracy                           0.85      2111\n",
      "   macro avg       0.85      0.85      0.85      2111\n",
      "weighted avg       0.85      0.85      0.85      2111\n",
      "\n",
      "[[901 158]\n",
      " [152 900]]\n"
     ]
    }
   ],
   "source": [
    "# Bayesian model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# model evaluation\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "classifier_nb=MultinomialNB(alpha=0.2)\n",
    "classifier_nb.fit(X_train_tfidf,y_train)\n",
    "y_train_pred=classifier_nb.predict(X_train_tfidf)\n",
    "y_valid_pred=classifier_nb.predict(X_valid_tfidf)\n",
    "y_test_pred=classifier_nb.predict(X_test_tfidf)\n",
    "\n",
    "print(classifier_nb.score(X_test_tfidf,y_test))\n",
    "print(classification_report(y_test,y_test_pred))\n",
    "print(confusion_matrix(y_test,y_test_pred))\n",
    "\n",
    "classifier_nb.fit(X_train_tfidf_chi,y_train)\n",
    "y_train_pred_chi=classifier_nb.predict(X_train_tfidf_chi)\n",
    "y_valid_pred_chi=classifier_nb.predict(X_valid_tfidf_chi)\n",
    "y_test_pred_chi=classifier_nb.predict(X_test_tfidf_chi)\n",
    "\n",
    "print(classifier_nb.score(X_test_tfidf_chi,y_test))\n",
    "print(classification_report(y_test,y_test_pred_chi))\n",
    "print(confusion_matrix(y_test,y_test_pred_chi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian sentiment analysis\n",
    "Build a classic **Bayesian** model to deal with text classification task, you can learn the basic steps to solve text classification problem by traditional ML way.\n",
    "\n",
    "### Packages\n",
    "- pandas\n",
    "- os\n",
    "- jieba\n",
    "- sklearn\n",
    "- tqdm\n",
    "- logging\n",
    "\n",
    "### Important functions\n",
    "- pandas.read_csv(path,sep='\\t',names)\n",
    "- sklearn.feature_extraction.text.TfidfVectorizer\n",
    "- sklearn.naive_bayes.MultinomialNB()\n",
    "- sklearn.feature_selection.SelectKBest\n",
    "- sklearn.feature_selection.chi2\n",
    "- sklearn.metrics.classification_report\n",
    "- sklearn.metrics.confusion_matrix\n",
    "\n",
    "### Main process\n",
    "- read and preprocess data\n",
    "- extract TF-IDF feature\n",
    "- select core features\n",
    "- build and train Bayesian model\n",
    "- evaluate the trained model \n",
    "\n",
    "### Dataset\n",
    "You can get the data from the [link](https://github.com/bojone/bert4keras/tree/master/examples/datasets), the dataset is divided into three parts:sentiment.train.data,sentiment.valid.data,sentiment.test.data\n",
    "\n",
    "### Run\n",
    "You can just run this program step by step and get a complete understanding as how to do text classification by traditional ML way.\n",
    "\n",
    "### Special code\n",
    "```python\n",
    "# the commonly used following code is mainly used to create the logger\n",
    "import logging\n",
    "import os\n",
    "logger=logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "data_path = \"./home/aistudio/data/sentiment\"\n",
    "log_path=os.path.join(data_path,'log.txt')\n",
    "# 日志记录到文件\n",
    "handler=logging.FileHandler(log_path)\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter=logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "# 日志打印到控制台\n",
    "console=logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "console.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(handler)\n",
    "logger.addHandler(console)\n",
    "logger.info('The logger is successfully built for the model')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
