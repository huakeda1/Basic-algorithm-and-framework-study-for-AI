{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./home/aistudio/work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code is mainly used to unzip the required data to specific folder.\n",
    "# import os,zipfile\n",
    "# src_file='home.zip'\n",
    "# zf=zipfile.ZipFile(src_file)\n",
    "# zf.extractall('./home')\n",
    "# zf.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  pytorch_model.bin\tvocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls ./home/aistudio/data/data56340"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modeling_bert.py  run_language_modeling.py  tokenization_bert.py  vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls ./home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.hidden_size=768\n",
    "        self.num_attention_heads=12\n",
    "        self.attention_probs_dropout_prob=0.1\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(BertPooler,self).__init__()\n",
    "        self.dense=nn.Linear(config.hidden_size,config.hidden_size)\n",
    "        self.activation=nn.Tanh()\n",
    "    def forward(self,hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        # hidden_states.shape 为[batch_size, seq_len, hidden_dim]\n",
    "        first_token_tensor=hidden_states[:,0]\n",
    "        pooled_output=self.dense(first_token_tensor)\n",
    "        pooled_output=self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=Config()\n",
    "bertpooler=BertPooler(config)\n",
    "input_tensor=torch.ones([8,50,768])\n",
    "output_tensor=bertpooler(input_tensor)\n",
    "assert output_tensor.shape==torch.Size([8,768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal function from tokenizer\n",
    "from typing import List,Optional,Tuple\n",
    "def build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]:\n",
    "    if token_ids_1 is None:\n",
    "        return [self.cls_token_id]+token_ids_0+[self.sep_token_id]\n",
    "    cls=[self.cls_token_id]\n",
    "    sep=[self.sep_token_id]\n",
    "    return cls + token_ids_0 + sep + token_ids_1 + sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 3614, 6816, 1920, 2157, 3341, 1168, 1400, 1322, 4415, 2339, 2110, 7368, 2110, 739, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3614, 6816, 1920, 2157, 3341, 1168, 1400, 1322, 4415, 2339, 2110, 7368, 2110, 739, 102, 6371, 6399, 3173, 3301, 1351, 3221, 671, 816, 2571, 727, 4638, 752, 2658, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[101, 3614, 6816, 1920, 2157, 3341, 1168, 1400, 1322, 4415, 2339, 2110, 7368, 2110, 739, 102, 6371, 6399, 3173, 3301, 1351, 3221, 671, 816, 2571, 727, 4638, 752, 2658, 119, 102]\n",
      "[101, 101, 3614, 6816, 1920, 2157, 3341, 1168, 1400, 1322, 4415, 2339, 2110, 7368, 2110, 739, 102, 6371, 6399, 3173, 3301, 1351, 3221, 671, 816, 2571, 727, 4638, 752, 2658, 119, 102, 102]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer=BertTokenizer.from_pretrained('./home/aistudio/data/data56340')\n",
    "inputs_1=tokenizer('欢迎大家来到后厂理工学院学习.')\n",
    "print(inputs_1)\n",
    "inputs_2=tokenizer('欢迎大家来到后厂理工学院学习','认识新朋友是一件快乐的事情.')\n",
    "print(inputs_2)\n",
    "inputs_3=tokenizer.encode('欢迎大家来到后厂理工学院学习','认识新朋友是一件快乐的事情.')\n",
    "print(inputs_3)\n",
    "inputs_4=tokenizer.build_inputs_with_special_tokens(inputs_3)\n",
    "print(inputs_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将每个输入的数据句子中15%的概率随机抽取token，在这15%中的80%概论将token替换成[MASK]，如上图所示，15%中的另外10%替换成其他token，比如把‘理’换成‘后’，15%中的最后10%保持不变，就是还是‘理’这个token。\n",
    "\n",
    "# 之所以采用三种不同的方式做mask，是因为后面的fine-tuning阶段并不会做mask的操作，为了减少pre-training和fine-tuning阶段输入分布不一致的问题，所以采用了这种策略。\n",
    "# MLM output layer definition\n",
    "class BertLMPredictionHead(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.transform=BertPredictionHeadTransform(config)\n",
    "        # 在nn.Linear操作过程中的权重和bert输入的embedding权重共享\n",
    "        # Embedding层和FC层权重共享，Embedding层中和向量 v 最接近的那一行对应的词，会获得更大的预测概率。\n",
    "        # 实际上，Embedding层和FC层有点像互为逆过程。\n",
    "        self.decoder=nn.Linear(config.hidden_size,config.vocab_size,bias=False)\n",
    "        self.bias=nn.Parameter(torch.zeros(config.vocab_size))\n",
    "        self.decoder.bias=self.bias\n",
    "    def forward(self,hidden_states):\n",
    "        hidden_states=self.transform(hidden_states)\n",
    "        hidden_states=self.decoder(hidden_states)\n",
    "        return hidden_states\n",
    "# 只考虑MLM任务，通过BertForMaskedLM完成预训练，loss为CrossEntropyLoss\n",
    "# 同时考虑MLM和NSP，通过BertForPreTraining完成预训练，loss为CrossEntropyLoss\n",
    "# as for NSP， self.seq_relationship=nn.Linear(config.hidden_size,2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAPT：领域自适应预训练(Domain-Adaptive Pretraining)\n",
    "# TAPT：任务自适应预训练(Task-Adaptive Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask token处理\n",
    "\"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
    "def mask_token(inputs:torch.Tensor,tokenizer:BertTokenizer,args)->Tuple[torch.Tensor,torch.Tensor]:\n",
    "    if tokenizer.mask_token is None:\n",
    "        raise ValueError('This tokenizer does not have a mask token which is necessary for masked language model. Remove the --mlm flag if you want to use this tokenizer.')\n",
    "    labels=inputs.clone()\n",
    "    # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "    probability_matrix=torch.full(labels.shape,args.mlm_probability)\n",
    "    # filter the exist special token which will not be masked anymore.\n",
    "    special_tokens_mask=[tokenizer.get_special_tokens_mask(val,already_has_special_tokens=True) for val in labels.tolist()]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask,dtype=torch.bool),value=0.0)\n",
    "    # filter the exist pad token which will not be masked anymore\n",
    "    if tokenizer.pad_token is not None:\n",
    "        padding_mask=labels.eq(tokenizer.pad_token_id)\n",
    "        probability_matrix.masked_fill_(padding_mask,value=0.0)\n",
    "    # get out the possible masked position with 1.0 which means 15% of all pure tokens will be picked out for relevant masking.\n",
    "    masked_indices=torch.bernoulli(probability_matrix).bool()\n",
    "    # we only need the masked position to compute loss while the other token ids are set to be -100\n",
    "    labels[~masked_indices]=-100\n",
    "    \n",
    "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "    indices_replaced=torch.bernoulli(torch.full(labels.shape,0.8)).bool()&masked_indices\n",
    "    inputs[indices_replaced]=tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "    \n",
    "    # 10% of the time, we replace masked input tokens with random word\n",
    "    indices_random=torch.bernoulli(torch.full(labels.shape,0.5)).bool()&masked_indices&~indices_replaced\n",
    "    random_words=torch.randint(len(tokenizer),labels.shape,dtype=torch.long)\n",
    "    inputs[indices_random]=random_words[indices_random]\n",
    "    \n",
    "    # The rest of the time(10%) we keep the masked input tokens unchanged\n",
    "    return inputs,labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 8578, 8318, 3221, 1825,  754, 4636, 2428, 3918, 2428, 2110,  739,\n",
      "         2398, 1378, 7607, 3444, 4638,  782, 2339, 3255, 5543, 2110,  739,  680,\n",
      "         2141, 6378, 4852, 1277, 8024, 2990,  897, 1762, 5296, 5356, 4923, 4384,\n",
      "         1862,  510, 1048, 6589, 9483, 5050, 1213,  510, 3862, 7030, 2458, 3975,\n",
      "         5050, 3791, 1469, 2458, 3123, 3144, 2945, 8024, 2376, 1221, 2458, 1355,\n",
      "         5442, 2571, 6862, 1158, 2456, 1469, 6956, 5392, 3563, 1798,  511,  102]])\n",
      "tensor([[  101,  8578,  8318,  3221,   103,   754,  4636,  2428,  3918,  2428,\n",
      "          2110,   739,  2398,  1378,  7607,  3444,  4638,   782,   103,  3255,\n",
      "          5543,  2110,   739, 12919,  2141,  6378,  4852,  1277,  8024,   103,\n",
      "           897,  1762,  5296,  5356,  4923,  4384,  1862,   510,  1048,  6589,\n",
      "          9483,   103,  1213,   510,  3862,  7030,  2458,   103,  5050,  3791,\n",
      "          1469,   103,  3123,  3144,  2945,  8024,  2376,  1221,  2458,  1355,\n",
      "           103,  2571,  6862,  1158,  2456,   103,  6956,  5392,  3563,  1798,\n",
      "           511,   102]])\n",
      "tensor([[-100, 8578, -100, -100, 1825, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, 2339, -100, -100, -100, -100,  680,\n",
      "         -100, -100, -100, -100, -100, 2990, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, 5050, -100, -100, -100, -100, -100, 3975,\n",
      "         -100, -100, -100, 2458, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         5442, -100, -100, -100, -100, 1469, -100, -100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer=BertTokenizer.from_pretrained('./home/aistudio/data/data56340')\n",
    "txt = 'AI Studio是基于百度深度学习平台飞桨的人工智能学习与实训社区，提供在线编程环境、免费GPU算力、海量开源算法和开放数据，帮助开发者快速创建和部署模型。'\n",
    "inputs_all=tokenizer(txt)\n",
    "pre_inputs=torch.tensor([inputs_all['input_ids']])\n",
    "print(pre_inputs)\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.mlm_probability = 0.15\n",
    "args=Args()\n",
    "inputs,labels=mask_token(pre_inputs,tokenizer,args)\n",
    "print(inputs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# large scale model training strategy\n",
    "\n",
    "# gradient accumulation\n",
    "# 一般在单卡GPU训练时采用，防止显存溢出\n",
    "if args.max_steps>0:\n",
    "    t_total=args.max_steps\n",
    "    args.num_train_epochs=args.max_steps//(len(train_dataloader)//args.gradient_accumulation_steps)+1\n",
    "else:\n",
    "    t_total=len(train_dataloader)//args.gradient_accumulation_steps*args.num_train_epochs\n",
    "    \n",
    "# for i, (inputs, labels) in enumerate(training_set):\n",
    "#   loss = model(inputs, labels)                    # 计算loss\n",
    "#   loss = loss / accumulation_steps                # Normalize our loss (if averaged)\n",
    "#   loss.backward()                                 # 反向计算梯度，累加到之前梯度上\n",
    "#   if (i+1) % accumulation_steps == 0:             \n",
    "#       optimizer.step()                            # 更新参数\n",
    "#       model.zero_grad()                           # 清空梯度\n",
    "\n",
    "# Nvidia 混合精度工具apex\n",
    "if args.fp16:\n",
    "    try:\n",
    "        from apex import amp\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training\")\n",
    "    model,optimizer=amp.initialize(model,optimizer,opt_level=args.fp16_opt_level)\n",
    "\n",
    "# multi-gpu training (should be after apex fp16 initialization)\n",
    "if args.n_gpu>1:\n",
    "    model=torch.nn.DataParallel(model)\n",
    "\n",
    "# distributed traing (should be after apex fp16 initialization)\n",
    "if args.local_rank != -1:\n",
    "    model=torch.nn.parallel.DistributedDataParallel(model,device_ids=[args.local_rank],output_device=args.local_rank,find_unused_parameters=True)\n",
    "    \n",
    "# 基于Transformer结构的大规模预训练模型预训练和微调都会采用wramup的方式\n",
    "# scheduler =get_linear_schedule_with_warmup(optimizer,num_warmup_steps=args.warmup_steps,num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在预训练模型训练的开始阶段，BERT模型对数据的初始分布理解很少，在第一轮训练的时候，模型的权重会迅速改变。如果一开始学习率很大，非常有可能对数据产生过拟合的学习，后面需要很多轮的训练才能弥补，会花费更多的训练时间。但模型训练一段时间后，模型对数据分布已经有了一定的学习，这时就可以提升学习率，能够使得模型更快的收敛，训练也更加稳定，这个过程就是warmup，学习率是从低逐渐增高的过程。\n",
    "# 当BERT模型训练一定时间后，尤其是后续快要收敛的时候，如果还是比较大的学习率，比较难以收敛，调低学习率能够更好的微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PreTrainedModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a13fb56f27bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mBertTokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_rank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtb_writer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PreTrainedModel' is not defined"
     ]
    }
   ],
   "source": [
    "# train process\n",
    "import os\n",
    "import tqdm\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO,format='%(asctime)s-%(name)s-%(levelname)s-%(message)s')\n",
    "logger=logging.getLogger(__name__)\n",
    "\n",
    "def train(args,train_dataset,model:PreTrainedModel,tokenizer:BertTokenizer)->Tuple[int,float]:\n",
    "    if args.local_rank in [-1,0]:\n",
    "        tb_writer=SummaryWriter()\n",
    "    args.train_batch_size=args.per_gpu_batch_size*max(1,args.n_gpu)\n",
    "    # 补齐 pad\n",
    "    def collate(examples:List[torch.tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples,batch_first=True)\n",
    "        return pad_sequence(examples,batch_first=True,padding_value=tokenizer.pad_token_id)\n",
    "    train_sampler=RandomSampler(train_dataset) if args.local_rank==-1 else DistributedSampler(train_dataset)\n",
    "    # create dataloader for training\n",
    "    train_dataloader=DataLoader(train_dataset,sampler=train_sampler,batch_size=args.train_batch_size,collate_fn=collate)\n",
    "    # prepare gradient accumulation\n",
    "    if args.max_steps>0:\n",
    "        t_total=args.max_steps\n",
    "        args.num_train_epochs=args.max_steps//(len(train_dataloader)//args.gradient_accumulation_steps)+1\n",
    "    else:\n",
    "        t_total=len(train_dataloader)//args.gradient_accumulation_steps*args.num_train_epochs\n",
    "    # load the model\n",
    "    model=model.module if hasattr(model,'module') else model # take care of distribute/parallel training\n",
    "    model.resize_token_embeddings(len(dataloader))\n",
    "    # Prepare optimizer and schedule(linear warmup and decay)\n",
    "    no_decay=['bias','LayerNorm.weight']\n",
    "    optimizer_grouped_parameters=[{'params':[p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)],'weight_decay':args.weight_decay},{'params':[p for n,p in model.named_parameters() if any(nd in n for nd in no_decay)],'weight_decay':0.0}]\n",
    "    optimizer=AdamW(optimizer_grouped_parameters,lr=args.learning_rate,eps=args.adam_epsilon)\n",
    "    scheduler=get_linear_schedule_with_warmup(optimizer,num_warmup_steps=args.warmup_steps,num_training_steps=t_total)\n",
    "    # check if saved optimizer or scheduler state exist\n",
    "    if (args.model_name_or_path \n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path,'optimizer.pt'))\n",
    "       and os.path.isfile(os.path.join(args.model_name_or_path,'scheduler.pt'))):\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path,'optimizer.pt')))\n",
    "        scheduler.load_state_dict(torch.laod(os.path.join(args.model_name_or_path,'scheduler.pt')))\n",
    "    # 混合精度训练\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n",
    "        model,optimizer=amp.initialize(model,optimizer,opt_level=args.fp16_opt_level)\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu>1:\n",
    "        model=torch.nn.DataParallel(model)\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank!=-1:\n",
    "        model=torch.nn.parallel.DistributedDataParallel(model,device_ids=[args.local_rank],output_device=args.local_rank,find_unused_parameters=True)\n",
    "    # display log information before training\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info('Num examples =%d',len(train_dataset))\n",
    "    logger.info(\"Num Epochs =%d\",args.num_train_epochs)\n",
    "    logger.info(\"Instantaneous batch size per GPU=%d\",args.per_gpu_batch_size)\n",
    "    logger.info(\"Total train batch size(w.parallel,distribute&accumulation)=%d\",\n",
    "                args.train_batch_size*args.gradient_accumulation_steps*\n",
    "                (torch.distributed.get_world_size() if args.local_rank!=-1 else 1),)\n",
    "    logger.info(\"Gradient Accumulation steps=%d\",args.gradient_accumulation_steps)\n",
    "    logger.info(\"Total optimization steps=%d\",t_total)\n",
    "    \n",
    "    global_step=0\n",
    "    epochs_trained=0\n",
    "    steps_trained_in_current_epoch=0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
    "        try:\n",
    "            # set global_step to global step of last saved checkpoint from model path\n",
    "            checkpoint_suffix=args.model_name_or_path.split('-')[-1].split('/')[0]\n",
    "            global_step=int(checkpoint_suffix)\n",
    "            epochs_trained=global_step//(len(train_dataloader)//args.gradient_accumulation_steps)\n",
    "            steps_trained_in_current_epoch=global_step%(len(train_dataloader)//args.gradient_accumulation_steps)\n",
    "            logger.info(\"Continuing training from checkpoint, will skip to saved global step\")\n",
    "            logger.info(\"Continuing training from epcoh %d\",epochs_trained)\n",
    "            logger.info(\"Continuing training from global step %d\",global_step)\n",
    "            logger.info(\"Will skip the first %d step in the first epoch\",steps_trained_in_current_epoch)\n",
    "        except ValueError:\n",
    "            logger.info(\" Starting fine_tuning\")\n",
    "    tr_loss,logging_loss=0.0,0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator=trange(epochs_trained,int(args.num_train_epochs),desc='Epoch',disable=args.local_rank not in [-1,0])\n",
    "    set_seed(args) # Added here for reproducibility\n",
    "    for epoch in train_iterator:\n",
    "        epoch_iterator=tqdm(train_dataloader,desc='Iteration',disable=args.local_rank not in [-1,0])\n",
    "        if args.local_rank!=-1:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "        for step,batch in enumerate(epoch_iterator):\n",
    "            # skip past any already trained step if resuming training\n",
    "            if steps_trained_in_current_epoch >0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "            # 对输入数据进行mask处理\n",
    "            inputs,labels=mask_tokens(batch,tokenizer,args) if args.mlm else (batch,batch)\n",
    "            inputs=inputs.to(args.device)\n",
    "            labels=labels.to(args.device)\n",
    "            model.train()\n",
    "            outputs=model(inputs,masked_lm_labels=labels) if args.mlm else model(inputs,labels=labels)\n",
    "            loss=outputs[0]\n",
    "            if args.n_gpu>1:\n",
    "                loss=loss.mean() # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps>1:\n",
    "                loss=loss/args.gradient_accumulation_steps\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss,optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            tr_loss+=loss.item()\n",
    "            if (step+1)%args.gradient_accumulation_steps==0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm(amp.master_params(optimizer),args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm(model.parameters(),args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "                global_step+=1\n",
    "            if agrs.local_rank in [-1,0] and args.logging_steps>0 and global_step%args.logging_steps==0:\n",
    "                # log metrics\n",
    "                if args.local_rank==-1 and args.evaluate_during_training:\n",
    "                    # only evaluate when single GPU otherwise metrics may not average well\n",
    "                    results=evaluate(args,model,tokenizer)\n",
    "                    for key,value in results.items():\n",
    "                        tb_writer.add_scaler(\"eval_{}\".format(key),value,global_step)\n",
    "                tb_writer.add_scaler('lr',scheduler.get_lr()[0],global_step)\n",
    "                tb_writer.add_scaler('loss',(tr_loss-logging_loss)/args.logging_steps,global_step)\n",
    "                logging_loss=tr_loss\n",
    "            if args.local_rank in [-1,0] and args.save_steps>0 and global_step%args.save_steps==0:\n",
    "                checkpoint_predix='checkpoint'\n",
    "                # save model check point\n",
    "                output_dir=os.path.join(args.outout_dir,\"{}-{}\".format(checkpoint_prefix,global_step))\n",
    "                os.makedirs(output_dir,exist_ok=True)\n",
    "                model_to_save=(model.module if hasattr(model,\"module\") else model)\n",
    "                model_to_save.save_pretrained(output_dir)\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "                torch.save(args,os.path.join(output_dir,'training_args.bin'))\n",
    "                logger.info('Saving model checkpoint to %s',output_dir)\n",
    "                \n",
    "                _rotate_checkpoints(args,checkpoint_prefix)\n",
    "                \n",
    "                torch.save(optimizer.state_dict(),os.path.join(output_dir,'optimizer.pt'))\n",
    "                torch.save(scheduler.state_dict(),os.path.join(output_dir,'scheduler.pt'))\n",
    "                logger.info('Saving optimizer and scheduler states to %s',output_dir)\n",
    "            if args.max_steps>0 and global_step>args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps>0 and global_step>args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "    if args.local_rank in [-1,0]:\n",
    "        tb_writer.close()\n",
    "    return global_step,tr_loss/global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter('runs/scalar_example')\n",
    "for i in range(10):\n",
    "    writer.add_scalar('quadratic', i**2, global_step=i)\n",
    "    writer.add_scalar('exponential', 2**i, global_step=i)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert pretraining skills study\n",
    "We study the skills around the **MLM** and **NSP** and learn how to do MLM and NSP, learn how to get **bertpool** output,learn how to use **BertTokenizer**, replicate the function of **mask token** operation, go through all the process of **training**.\n",
    "\n",
    "### Main Content\n",
    "- How to create and use BertPooler \n",
    "- How to use BertTokenizer\n",
    "- Reference code for understanding BertForMaskedLM\n",
    "- Introduction about DAPT and TAPT\n",
    "- How to mask token for MLM\n",
    "- Large scale model training strategy\n",
    "- Learn the whole training code and process\n",
    "\n",
    "### Packages\n",
    "- torch\n",
    "- transformers\n",
    "- typing\n",
    "- apex\n",
    "- logging\n",
    "- tensorboardX\n",
    "- tqdm\n",
    "\n",
    "### Important functions\n",
    "- nn.Module\n",
    "- nn.Linear()\n",
    "- nn.parameter()\n",
    "- torch.full()\n",
    "- torch.eq()\n",
    "- torch.tensor(dtype=torch.bool)\n",
    "- torch.masked_fill()\n",
    "- torch.bernoulli()\n",
    "- torch.randint()\n",
    "- torch.nn.DataParallel()\n",
    "- torch.nn.parallel.DistributedParallel()\n",
    "- torch.utils.data.DataLoader()\n",
    "- torch.nn.utils.rnn.pad_sequence()\n",
    "- hasattr()\n",
    "- AdamW()\n",
    "- logging.getLogger().info()\n",
    "- logging.basicConfig()\n",
    "- SummaryWriter().add_scaler()\n",
    "- SummaryWriter().close()\n",
    "- os.path.isfile()\n",
    "- os.path.join()\n",
    "- for step,batch in enumerate(tqdm(dataloader))\n",
    "- trange means tqdm(range())\n",
    "- epoch_iterator.close()\n",
    "- loss.backward()\n",
    "- torch.nn.utils.clip_grad_norm_()\n",
    "- optimizer.step()\n",
    "- scheduler.step()\n",
    "- model.zero_grad()\n",
    "- os.makedirs(exist_ok=True)\n",
    "- model.save_pretrained()\n",
    "- tokenizer.save_pretrained()\n",
    "- torch.save(optimizer.state_dict(),filedir)\n",
    "- get_linear_schedule_with_warmup(optimizer,num_warmup_steps,num_training_steps)\n",
    "\n",
    "### Special code\n",
    "```python\n",
    "# class BertLMPredictionHead segment \n",
    "self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "self.decoder.bias = self.bias\n",
    "\n",
    "# mask token segment\n",
    "probability_matrix = torch.full(labels.shape, args.mlm_probability)\n",
    "special_tokens_mask = [\n",
    "    tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "]\n",
    "probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "if tokenizer._pad_token is not None:\n",
    "    padding_mask = labels.eq(tokenizer.pad_token_id)\n",
    "    probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "# train process\n",
    "\n",
    "# 补齐pad and create dataloader\n",
    "def collate(examples: List[torch.Tensor]):\n",
    "    if tokenizer._pad_token is None:\n",
    "        return pad_sequence(examples, batch_first=True)\n",
    "    return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate\n",
    ")\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    ")\n",
    "\n",
    "# Load in optimizer and scheduler states\n",
    "optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
