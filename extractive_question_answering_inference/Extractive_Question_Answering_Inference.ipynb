{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering,AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForQuestionAnswering.from_pretrained('./pretrained_model',return_dict=True)\n",
    "tokenizer=AutoTokenizer.from_pretrained('./pretrained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=r\"\"\"\n",
    "ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"How many pretrained models are available in ðŸ¤— Transformers?\",\"What does ðŸ¤— Transformers provide?\",\"ðŸ¤— Transformers provides interoperability between which frameworks?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:['[CLS]', 'how', 'many', 'pre', '##train', '##ed', 'models', 'are', 'available', 'in', '[UNK]', 'transformers', '?', '[SEP]', '[UNK]', 'transformers', '(', 'formerly', 'known', 'as', 'p', '##yt', '##or', '##ch', '-', 'transformers', 'and', 'p', '##yt', '##or', '##ch', '-', 'pre', '##train', '##ed', '-', 'bert', ')', 'provides', 'general', '-', 'purpose', 'architecture', '##s', '(', 'bert', ',', 'gp', '##t', '-', '2', ',', 'roberta', ',', 'xl', '##m', ',', 'di', '##sti', '##lbert', ',', 'xl', '##net', 'â€¦', ')', 'for', 'natural', 'language', 'understanding', '(', 'nl', '##u', ')', 'and', 'natural', 'language', 'generation', '(', 'nl', '##g', ')', 'with', 'over', '32', '+', 'pre', '##train', '##ed', 'models', 'in', '100', '+', 'languages', 'and', 'deep', 'inter', '##oper', '##ability', 'between', 'tensor', '##flow', '2', '.', '0', 'and', 'p', '##yt', '##or', '##ch', '.', '[SEP]']\n",
      "Question:How many pretrained models are available in ðŸ¤— Transformers?\n",
      "Start:82,End:85\n",
      "Answer:over 32 +\n",
      "Text:['[CLS]', 'what', 'does', '[UNK]', 'transformers', 'provide', '?', '[SEP]', '[UNK]', 'transformers', '(', 'formerly', 'known', 'as', 'p', '##yt', '##or', '##ch', '-', 'transformers', 'and', 'p', '##yt', '##or', '##ch', '-', 'pre', '##train', '##ed', '-', 'bert', ')', 'provides', 'general', '-', 'purpose', 'architecture', '##s', '(', 'bert', ',', 'gp', '##t', '-', '2', ',', 'roberta', ',', 'xl', '##m', ',', 'di', '##sti', '##lbert', ',', 'xl', '##net', 'â€¦', ')', 'for', 'natural', 'language', 'understanding', '(', 'nl', '##u', ')', 'and', 'natural', 'language', 'generation', '(', 'nl', '##g', ')', 'with', 'over', '32', '+', 'pre', '##train', '##ed', 'models', 'in', '100', '+', 'languages', 'and', 'deep', 'inter', '##oper', '##ability', 'between', 'tensor', '##flow', '2', '.', '0', 'and', 'p', '##yt', '##or', '##ch', '.', '[SEP]']\n",
      "Question:What does ðŸ¤— Transformers provide?\n",
      "Start:33,End:38\n",
      "Answer:general - purpose architectures\n",
      "Text:['[CLS]', '[UNK]', 'transformers', 'provides', 'inter', '##oper', '##ability', 'between', 'which', 'framework', '##s', '?', '[SEP]', '[UNK]', 'transformers', '(', 'formerly', 'known', 'as', 'p', '##yt', '##or', '##ch', '-', 'transformers', 'and', 'p', '##yt', '##or', '##ch', '-', 'pre', '##train', '##ed', '-', 'bert', ')', 'provides', 'general', '-', 'purpose', 'architecture', '##s', '(', 'bert', ',', 'gp', '##t', '-', '2', ',', 'roberta', ',', 'xl', '##m', ',', 'di', '##sti', '##lbert', ',', 'xl', '##net', 'â€¦', ')', 'for', 'natural', 'language', 'understanding', '(', 'nl', '##u', ')', 'and', 'natural', 'language', 'generation', '(', 'nl', '##g', ')', 'with', 'over', '32', '+', 'pre', '##train', '##ed', 'models', 'in', '100', '+', 'languages', 'and', 'deep', 'inter', '##oper', '##ability', 'between', 'tensor', '##flow', '2', '.', '0', 'and', 'p', '##yt', '##or', '##ch', '.', '[SEP]']\n",
      "Question:ðŸ¤— Transformers provides interoperability between which frameworks?\n",
      "Start:98,End:108\n",
      "Answer:tensorflow 2 . 0 and pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "for question in questions:\n",
    "    inputs=tokenizer(question,text,add_special_tokens=True,return_tensors='pt')\n",
    "    input_ids=inputs['input_ids'].tolist()[0]\n",
    "    text_tokens=tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    result=model(**inputs)\n",
    "    answer_start_scores,answer_end_scores=result.start_logits,result.end_logits\n",
    "    \n",
    "    answer_start_greed=torch.argmax(answer_start_scores,dim=-1)\n",
    "    answer_end_greed=torch.argmax(answer_end_scores,dim=-1)+1\n",
    "    # here and false is only used to test the abnormal situations that the prediction of end index is smaller than that of start index\n",
    "    if answer_start_greed<=answer_end_greed and False:\n",
    "        answer_start=answer_start_greed\n",
    "        answer_end=answer_end_greed\n",
    "    else:\n",
    "        q_len=len(tokenizer.encode(question,add_special_tokens=True))\n",
    "        answer_start_probs=torch.softmax(answer_start_scores,dim=-1)[0,q_len:-1]\n",
    "        answer_end_probs=torch.softmax(answer_end_scores,dim=-1)[0,q_len:-1]\n",
    "        start_end,score=None,-1\n",
    "        max_a_len=20\n",
    "        for start,start_p in enumerate(answer_start_probs):\n",
    "            for end,end_p in enumerate(answer_end_probs):\n",
    "                if end>=start and end<start+max_a_len:\n",
    "                    if start_p*end_p>score:\n",
    "                        score=start_p*end_p\n",
    "                        start_end=(start,end)\n",
    "        relative_start,relative_end=start_end\n",
    "        answer_start=relative_start+q_len\n",
    "        answer_end=relative_end+q_len+1\n",
    "    answer=tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    print(f'Text:{text_tokens}')\n",
    "    print(f'Question:{question}')\n",
    "    print(f'Start:{answer_start},End:{answer_end}')\n",
    "    print(f'Answer:{answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:['[CLS]', 'how', 'many', 'pre', '##train', '##ed', 'models', 'are', 'available', 'in', '[UNK]', 'transformers', '?', '[SEP]', '[UNK]', 'transformers', '(', 'formerly', 'known', 'as', 'p', '##yt', '##or', '##ch', '-', 'transformers', 'and', 'p', '##yt', '##or', '##ch', '-', 'pre', '##train', '##ed', '-', 'bert', ')', 'provides', 'general', '-', 'purpose', 'architecture', '##s', '(', 'bert', ',', 'gp', '##t', '-', '2', ',', 'roberta', ',', 'xl', '##m', ',', 'di', '##sti', '##lbert', ',', 'xl', '##net', 'â€¦', ')', 'for', 'natural', 'language', 'understanding', '(', 'nl', '##u', ')', 'and', 'natural', 'language', 'generation', '(', 'nl', '##g', ')', 'with', 'over', '32', '+', 'pre', '##train', '##ed', 'models', 'in', '100', '+', 'languages', 'and', 'deep', 'inter', '##oper', '##ability', 'between', 'tensor', '##flow', '2', '.', '0', 'and', 'p', '##yt', '##or', '##ch', '.', '[SEP]']\n",
      "Question:How many pretrained models are available in ðŸ¤— Transformers?\n",
      "Start:tensor([82]),End:tensor([85])\n",
      "Answer:over 32 +\n",
      "Text:['[CLS]', 'what', 'does', '[UNK]', 'transformers', 'provide', '?', '[SEP]', '[UNK]', 'transformers', '(', 'formerly', 'known', 'as', 'p', '##yt', '##or', '##ch', '-', 'transformers', 'and', 'p', '##yt', '##or', '##ch', '-', 'pre', '##train', '##ed', '-', 'bert', ')', 'provides', 'general', '-', 'purpose', 'architecture', '##s', '(', 'bert', ',', 'gp', '##t', '-', '2', ',', 'roberta', ',', 'xl', '##m', ',', 'di', '##sti', '##lbert', ',', 'xl', '##net', 'â€¦', ')', 'for', 'natural', 'language', 'understanding', '(', 'nl', '##u', ')', 'and', 'natural', 'language', 'generation', '(', 'nl', '##g', ')', 'with', 'over', '32', '+', 'pre', '##train', '##ed', 'models', 'in', '100', '+', 'languages', 'and', 'deep', 'inter', '##oper', '##ability', 'between', 'tensor', '##flow', '2', '.', '0', 'and', 'p', '##yt', '##or', '##ch', '.', '[SEP]']\n",
      "Question:What does ðŸ¤— Transformers provide?\n",
      "Start:tensor([33]),End:tensor([38])\n",
      "Answer:general - purpose architectures\n",
      "Text:['[CLS]', '[UNK]', 'transformers', 'provides', 'inter', '##oper', '##ability', 'between', 'which', 'framework', '##s', '?', '[SEP]', '[UNK]', 'transformers', '(', 'formerly', 'known', 'as', 'p', '##yt', '##or', '##ch', '-', 'transformers', 'and', 'p', '##yt', '##or', '##ch', '-', 'pre', '##train', '##ed', '-', 'bert', ')', 'provides', 'general', '-', 'purpose', 'architecture', '##s', '(', 'bert', ',', 'gp', '##t', '-', '2', ',', 'roberta', ',', 'xl', '##m', ',', 'di', '##sti', '##lbert', ',', 'xl', '##net', 'â€¦', ')', 'for', 'natural', 'language', 'understanding', '(', 'nl', '##u', ')', 'and', 'natural', 'language', 'generation', '(', 'nl', '##g', ')', 'with', 'over', '32', '+', 'pre', '##train', '##ed', 'models', 'in', '100', '+', 'languages', 'and', 'deep', 'inter', '##oper', '##ability', 'between', 'tensor', '##flow', '2', '.', '0', 'and', 'p', '##yt', '##or', '##ch', '.', '[SEP]']\n",
      "Question:ðŸ¤— Transformers provides interoperability between which frameworks?\n",
      "Start:tensor([98]),End:tensor([108])\n",
      "Answer:tensorflow 2 . 0 and pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "for question in questions:\n",
    "    inputs=tokenizer(question,text,add_special_tokens=True,return_tensors='pt')\n",
    "    input_ids=inputs['input_ids'].tolist()[0]\n",
    "    text_tokens=tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    result=model(**inputs)\n",
    "    answer_start_scores,answer_end_scores=result.start_logits,result.end_logits\n",
    "    \n",
    "    answer_start_greed=torch.argmax(answer_start_scores,dim=-1)\n",
    "    answer_end_greed=torch.argmax(answer_end_scores,dim=-1)+1\n",
    "    if answer_start_greed<=answer_end_greed:\n",
    "        answer_start=answer_start_greed\n",
    "        answer_end=answer_end_greed\n",
    "    else:\n",
    "        q_len=len(tokenizer.encode(question,add_special_tokens=True))\n",
    "        answer_start_probs=torch.softmax(answer_start_scores,dim=-1)[0,q_len:-1]\n",
    "        answer_end_probs=torch.softmax(answer_end_scores,dim=-1)[0,q_len:-1]\n",
    "        start_end,score=None,-1\n",
    "        max_a_len=20\n",
    "        for start,start_p in enumerate(answer_start_probs):\n",
    "            for end,end_p in enumerate(answer_end_probs):\n",
    "                if end>=start and end<start+max_a_len:\n",
    "                    if start_p*end_p>score:\n",
    "                        score=start_p*end_p\n",
    "                        start_end=(start,end)\n",
    "        answer_start,answer_end=start_end\n",
    "        answer_end+=1\n",
    "    answer=tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    print(f'Text:{text_tokens}')\n",
    "    print(f'Question:{question}')\n",
    "    print(f'Start:{answer_start},End:{answer_end}')\n",
    "    print(f'Answer:{answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 110])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_end_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Question Answering(Inference)\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the **SQuAD** dataset, which is entirely based on that task. If you would like to fine-tune a model on a SQuAD task, you may leverage the **run_squad.py** and **run_tf_squad.py** scripts.\n",
    "\n",
    "## Packages\n",
    "- Transformers 3.5.0\n",
    "- Torch\n",
    "\n",
    "## The process is the following:\n",
    "- 1) Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it with the weights stored in the checkpoint.\n",
    "\n",
    "- 2) Define a text and a few questions.\n",
    "\n",
    "- 3) Iterate over the questions and build a sequence from the text and the current question, with the correct model-specific separators token type ids and attention masks.\n",
    "\n",
    "- 4) Pass this sequence through the model. This outputs a range of scores across the entire sequence tokens (question and text), for both the start and end positions.\n",
    "\n",
    "- 5) Compute the softmax of the result to get probabilities over the tokens.\n",
    "\n",
    "- 6) Fetch the tokens from the identified start and stop values, convert those tokens to a string.\n",
    "\n",
    "- 7) Print the results.\n",
    "\n",
    "## Pretrained model\n",
    "You can download the pretrained weights from the [link](https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/tree/main)\n",
    "\n",
    "## Special code\n",
    "```python\n",
    "for question in questions:\n",
    "    inputs=tokenizer(question,text,add_special_tokens=True,return_tensors='pt')\n",
    "    input_ids=inputs['input_ids'].tolist()[0]\n",
    "    text_tokens=tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    result=model(**inputs)\n",
    "    answer_start_scores,answer_end_scores=result.start_logits,result.end_logits\n",
    "    \n",
    "    answer_start_greed=torch.argmax(answer_start_scores,dim=-1)\n",
    "    answer_end_greed=torch.argmax(answer_end_scores,dim=-1)\n",
    "\n",
    "    if answer_start_greed<=answer_end_greed:\n",
    "        answer_start=answer_start_greed\n",
    "        answer_end=answer_end_greed+1\n",
    "    else:\n",
    "        q_len=len(tokenizer.encode(question,add_special_tokens=True))\n",
    "        answer_start_probs=torch.softmax(answer_start_scores,dim=-1)[0,q_len:-1]\n",
    "        answer_end_probs=torch.softmax(answer_end_scores,dim=-1)[0,q_len:-1]\n",
    "        start_end,score=None,-1\n",
    "        max_a_len=20\n",
    "        for start,start_p in enumerate(answer_start_probs):\n",
    "            for end,end_p in enumerate(answer_end_probs):\n",
    "                if end>=start and end<start+max_a_len:\n",
    "                    if start_p*end_p>score:\n",
    "                        score=start_p*end_p\n",
    "                        start_end=(start,end)\n",
    "        relative_start,relative_end=start_end\n",
    "        answer_start=relative_start+q_len\n",
    "        answer_end=relative_end+q_len+1\n",
    "    answer=tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
