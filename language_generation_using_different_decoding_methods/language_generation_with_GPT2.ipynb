{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel,GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer=GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model=TFGPT2LMHeadModel.from_pretrained('gpt2',pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos_token_id:50256\n",
      "eos_token_id:50256\n",
      "pad_token_id:None\n",
      "unk_token_id:50256\n",
      "cls_token_id:None\n",
      "sep_token_id:None\n",
      "mask_token_id:None\n"
     ]
    }
   ],
   "source": [
    "print('bos_token_id:{}'.format(tokenizer.bos_token_id))\n",
    "print('eos_token_id:{}'.format(tokenizer.eos_token_id))\n",
    "print('pad_token_id:{}'.format(tokenizer.pad_token_id))\n",
    "print('unk_token_id:{}'.format(tokenizer.unk_token_id))\n",
    "print('cls_token_id:{}'.format(tokenizer.cls_token_id))\n",
    "print('sep_token_id:{}'.format(tokenizer.sep_token_id))\n",
    "print('mask_token_id:{}'.format(tokenizer.mask_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[   40  2883  6155   351   616 13779  3290]], shape=(1, 7), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[   40  2883  6155   351   616 13779  3290    11   475   314  1101   407\n",
      "   1654   611   314  1183  1683   307  1498   284  2513   351   616  3290\n",
      "     13   314  1101   407  1654   611   314  1183  1683   307  1498   284\n",
      "   2513   351   616  3290    13   198   198    40  1101   407  1654   611\n",
      "    314  1183]], shape=(1, 50), dtype=int32)\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure if I'll\n"
     ]
    }
   ],
   "source": [
    "input_ids=tokenizer.encode('I enjoy walking with my cute dog',return_tensors='tf')\n",
    "print(input_ids)\n",
    "greedy_output=model.generate(input_ids,max_length=50,)\n",
    "print(greedy_output)\n",
    "print('Output:\\n'+100*'-')\n",
    "print(tokenizer.decode(greedy_output[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again. I'm not sure if I'll\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output=model.generate(input_ids,max_length=50,num_beams=5,early_stopping=True)\n",
    "print('Output:\\n'+100*'-')\n",
    "print(tokenizer.decode(beam_output[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n",
      "1: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to get back to\n",
      "2: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n",
      "3: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to get back to\n",
      "4: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a step\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_outputs=model.generate(input_ids,max_length=50,num_beams=5,no_repeat_ngram_size=2,num_return_sequences=5,early_stopping=True)\n",
    "print('Output:\\n'+100*'-')\n",
    "for i,beam_output in enumerate(beam_outputs):\n",
    "    print(\"{}: {}\".format(i,tokenizer.decode(beam_output,skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog again.\n",
      "\n",
      "I'm not sure how long I'll be able to walk without my dog.\n",
      "\n",
      "I don't know\n",
      "1: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if it's because I'm too young to walk with my dog, or because I'm too\n",
      "2: I enjoy walking with my cute dog, and I love to play with my dog. I'm a big fan of playing with my dog, and I'm also a big fan of being with my dog.\n",
      "\n",
      "I'm not sure if I'm\n",
      "3: I enjoy walking with my cute dog, but I'm not sure if I'd be able to walk with my dog if I didn't have a dog.\n",
      "\n",
      "I'm not sure if it's because I don't have a dog, or if\n",
      "4: I enjoy walking with my cute dog, and I love to play with my dog. I love to watch my dog play with me, and I like to play with my dogs.\n",
      "\n",
      "I love to spend time with my dog, and I like\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(2021)\n",
    "beam_sample_outputs=model.generate(input_ids,max_length=50,num_beams=5,no_repeat_ngram_size=5,do_sample=True,top_k=30,temperature=0.7,top_p=0.95,num_return_sequences=5,early_stopping=True)\n",
    "print('Output:\\n'+100*'-')\n",
    "for i,beam_sample_output in enumerate(beam_sample_outputs):\n",
    "    print(\"{}: {}\".format(i,tokenizer.decode(beam_sample_output,skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
