{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification,AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"O\",       # Outside of a named entity\n",
    "\"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n",
    "\"I-MISC\",  # Miscellaneous entity\n",
    "\"B-PER\",   # Beginning of a person's name right after another person's name\n",
    "\"I-PER\",   # Person's name\n",
    "\"B-ORG\",   # Beginning of an organisation right after another organisation\n",
    "\"I-ORG\",   # Organisation\n",
    "\"B-LOC\",   # Beginning of a location right after another location\n",
    "\"I-LOC\"    # Location\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 9])\n",
      "torch.Size([1, 32])\n",
      "[('[CLS]', 'O'), ('Hu', 'I-ORG'), ('##gging', 'I-ORG'), ('Face', 'I-ORG'), ('Inc', 'I-ORG'), ('.', 'O'), ('is', 'O'), ('a', 'O'), ('company', 'O'), ('based', 'O'), ('in', 'O'), ('New', 'I-LOC'), ('York', 'I-LOC'), ('City', 'I-LOC'), ('.', 'O'), ('Its', 'O'), ('headquarters', 'O'), ('are', 'O'), ('in', 'O'), ('D', 'I-LOC'), ('##UM', 'I-LOC'), ('##BO', 'I-LOC'), (',', 'O'), ('therefore', 'O'), ('very', 'O'), ('close', 'O'), ('to', 'O'), ('the', 'O'), ('Manhattan', 'I-LOC'), ('Bridge', 'I-LOC'), ('.', 'O'), ('[SEP]', 'O')]\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForTokenClassification.from_pretrained('./pretrained_model',return_dict=True)\n",
    "tokenizer=AutoTokenizer.from_pretrained('./pretrained_model')\n",
    "tokens=tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
    "inputs=tokenizer.encode(sequence,return_tensors='pt')\n",
    "outputs=model(inputs).logits\n",
    "# outputs shape (1,32,9)\n",
    "print(outputs.shape)\n",
    "predictions=outputs.argmax(dim=2)\n",
    "# predictions shape (1,32)\n",
    "print(predictions.shape)\n",
    "print([(token,label_list[prediction]) for token,prediction in zip(tokens,predictions[0].numpy())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition(Inference)\n",
    "Named Entity Recognition (NER) is the task of classifying tokens according to a class, for example, identifying a token as a person, an organisation or a location. An example of a named entity recognition dataset is the CoNLL-2003 dataset, which is entirely based on that task. If you would like to fine-tune a model on an NER task, you may leverage the **run_ner.py (PyTorch)**, **run_pl_ner.py (leveraging pytorch-lightning)** or the **run_tf_ner.py (TensorFlow)** scripts.\n",
    "\n",
    "## Packages\n",
    "- Transformers 3.5.0\n",
    "- Torch\n",
    "\n",
    "## The process is the following:\n",
    "- 1) Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it with the weights stored in the checkpoint.\n",
    "\n",
    "- 2) Define the label list with which the model was trained on.\n",
    "\n",
    "- 3) Define a sequence with known entities, such as “Hugging Face” as an organisation and “New York City” as a location.\n",
    "\n",
    "- 4) Split words into tokens so that they can be mapped to predictions. We use a small hack by, first, completely encoding and decoding the sequence, so that we’re left with a string that contains the special tokens.\n",
    "\n",
    "- 5) Encode that sequence into IDs (special tokens are added automatically).\n",
    "\n",
    "- 6) Retrieve the predictions by passing the input to the model and getting the first output. This results in a distribution over the 9 possible classes for each token. We take the argmax to retrieve the most likely class for each token.\n",
    "\n",
    "- 7) Zip together each token with its prediction and print it.\n",
    "\n",
    "## Pretrained model\n",
    "You can download the pretrained weights from the [link](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n",
    "\n",
    "## Special code\n",
    "```python\n",
    "model=AutoModelForTokenClassification.from_pretrained('./pretrained_model',return_dict=True)\n",
    "tokenizer=AutoTokenizer.from_pretrained('./pretrained_model')\n",
    "tokens=tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
    "inputs=tokenizer.encode(sequence,return_tensors='pt')\n",
    "outputs=model(inputs).logits\n",
    "# outputs shape (1,32,9)\n",
    "print(outputs.shape)\n",
    "predictions=outputs.argmax(dim=2)\n",
    "# predictions shape (1,32)\n",
    "print(predictions.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
