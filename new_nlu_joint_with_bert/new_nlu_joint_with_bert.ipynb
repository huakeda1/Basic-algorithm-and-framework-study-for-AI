{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict,Counter,defaultdict,Counter\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import torch\n",
    "from transformers import BertTokenizer,BertModel,get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from transformers import BertConfig,BertTokenizer,BertModel,AdamW\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "seed = 2021\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tags(tokenizer,word_seq,slots):\n",
    "    tag_seq=['O']*len(word_seq)\n",
    "    # attention please here value should be considered to be a list, because some recommends can be shown in one quest.\n",
    "    for key,values in slots.items():\n",
    "        for value in values:\n",
    "            current_slot_value=tokenizer.tokenize(value)\n",
    "            for i in range(len(word_seq)):\n",
    "                if word_seq[i:i+len(current_slot_value)]==current_slot_value:\n",
    "                    tag_seq[i]='B+'+key\n",
    "                    if len(current_slot_value)>1:\n",
    "                        tag_seq[i+1:i+len(current_slot_value)]=['I+'+key]*(len(current_slot_value)-1)\n",
    "                    break\n",
    "    return tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file_dir,tokenizer,include_sys=False):\n",
    "    data_key=['train','val','test']\n",
    "    intent_vocab=[]\n",
    "    tag_vocab=[]\n",
    "    for key in data_key:\n",
    "        file_name=os.path.join(file_dir,key+'.json.zip')\n",
    "        zpf=zipfile.ZipFile(file_name,'r')\n",
    "        data=json.load(zpf.open(key+'.json'))\n",
    "        sessions=[]\n",
    "        for num,session in data.items():\n",
    "            for i,message in enumerate(session[\"messages\"]):\n",
    "                utterance=message[\"content\"]\n",
    "                word_seq=tokenizer.tokenize(utterance)\n",
    "                if message[\"role\"]==\"sys\" and not include_sys:\n",
    "                    pass\n",
    "                else:\n",
    "                    processed_data=[]\n",
    "                    slots={}\n",
    "                    intents=[]\n",
    "                    golden=[]\n",
    "                    for intent,domain,slot,value in message[\"dialog_act\"]:\n",
    "                        if intent in ['Inform','Recommend'] and '酒店设施' not in slot:\n",
    "                            if value in utterance:\n",
    "                                idx=utterance.index(value)\n",
    "                                idx=len(tokenizer.tokenize(utterance[:idx]))\n",
    "                                new_value=''.join(word_seq[idx:idx+len(tokenizer.tokenize(value))])\n",
    "                                new_value=new_value.replace('##','')\n",
    "                                golden.append([intent,domain,slot,new_value])\n",
    "                                \n",
    "                                slot_name=\"+\".join([intent,domain,slot])\n",
    "                                if slot_name not in slots:\n",
    "                                    slots[slot_name]=[value]\n",
    "                                else:\n",
    "                                    slots[slot_name].append(value)\n",
    "                            else:\n",
    "                                golden.append([intent,domain,slot,value])\n",
    "                        else:\n",
    "                            intent_name='+'.join([intent,domain,slot,value])\n",
    "                            intents.append(intent_name)\n",
    "                            intent_vocab.append(intent_name)\n",
    "                            golden.append([intent,domain,slot,value])                        \n",
    "                    tag_seq=generate_tags(tokenizer,word_seq,slots)\n",
    "                    tag_vocab+=tag_seq\n",
    "                    processed_data.append(word_seq)\n",
    "                    processed_data.append(tag_seq)\n",
    "                    processed_data.append(intents)\n",
    "                    processed_data.append(golden)\n",
    "                    # attention please copy.deepcopy should be used to prevent data change later effect\n",
    "                    current_context=[item[\"content\"] for item in session[\"messages\"][0:i] ]\n",
    "#                     if len(current_context)==0:current_context=['']\n",
    "                    processed_data.append(current_context)\n",
    "                    sessions.append(processed_data)\n",
    "        with open(os.path.join(file_dir,f'formated_{key}_nlu_data.json'),\"w\",encoding='utf-8') as g:\n",
    "            json.dump(sessions,g,indent=2,ensure_ascii=False)\n",
    "        print(os.path.join(file_dir,f'formated_{key}_nlu_data.json'))\n",
    "    with open(os.path.join(file_dir,'intent_vocab.json'),\"w\",encoding='utf-8') as h:\n",
    "        output_intent_vocab=[x[0] for x in dict(Counter(intent_vocab)).items()]\n",
    "        json.dump(output_intent_vocab,h,indent=2,ensure_ascii=False)\n",
    "    print(os.path.join(file_dir,'intent_vocab.json'))\n",
    "    with open(os.path.join(file_dir,'tag_vocab.json'),\"w\",encoding='utf-8') as j:\n",
    "        output_tag_vocab=[x[0] for x in dict(Counter(tag_vocab)).items()]\n",
    "        json.dump(output_tag_vocab,j,indent=2,ensure_ascii=False)\n",
    "    print(os.path.join(file_dir,'tag_vocab.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    def __init__(self, intent_vocab_path, tag_vocab_path, pretrained_weights, max_history=3):\n",
    "        \"\"\"\n",
    "        :param intent_vocab: list of all intents\n",
    "        :param tag_vocab: list of all tags\n",
    "        :param pretrained_weights: which bert_policy, e.g. 'bert_policy-base-uncased'\n",
    "        \"\"\"\n",
    "        with open(intent_vocab_path,'r',encoding='utf-8') as f:\n",
    "            self.intent_vocab=json.load(f)\n",
    "        with open(tag_vocab_path,'r',encoding='utf-8') as g:\n",
    "            self.tag_vocab=json.load(g)\n",
    "        self.intent_dim = len(self.intent_vocab)\n",
    "        self.tag_dim = len(self.tag_vocab)\n",
    "        self.id2intent = dict([(i, x) for i, x in enumerate(self.intent_vocab)])\n",
    "        self.intent2id = dict([(x, i) for i, x in enumerate(self.intent_vocab)])\n",
    "        self.id2tag = dict([(i, x) for i, x in enumerate(self.tag_vocab)])\n",
    "        self.tag2id = dict([(x, i) for i, x in enumerate(self.tag_vocab)])\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "        self.data = {}\n",
    "        self.intent_weight = [1] * len(self.intent2id)\n",
    "        self.max_history=max_history\n",
    "        self.max_sen_len=0\n",
    "        self.max_context_len=0\n",
    "\n",
    "    def load_data(self, data_path, data_key, cut_sen_len=0):\n",
    "        \"\"\"\n",
    "        sample representation: [list of words, list of tags, list of intents, original dialog act]\n",
    "        :param data_key: train/val/tests\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # data是[tokens, tags, intents, raw_dialog_act, context[-context_size:]]]五个纬度的嵌套列表\n",
    "        # tokens是jieba切分的得到的词，tags是可以看作词对应的slot标签，\n",
    "        with open(data_path,'r',encoding='utf-8') as f:\n",
    "            self.data[data_key]=json.load(f)\n",
    "        max_context_len=0\n",
    "        max_sen_len=0\n",
    "        for d in self.data[data_key]:\n",
    "            # d = (tokens, tags, intents, raw_dialog_act, context(list of str))\n",
    "            if cut_sen_len > 0:\n",
    "                d[0] = d[0][:cut_sen_len]\n",
    "                d[1] = d[1][:cut_sen_len]\n",
    "                d[4] = [\" \".join(s.split()[:cut_sen_len]) for s in d[4][-self.max_history:]]\n",
    "\n",
    "            d[4] = self.tokenizer.encode(\"[CLS] \" + \" [SEP] \".join(d[4]))\n",
    "            \n",
    "            max_context_len = max(max_context_len, len(d[4]))\n",
    "            word_seq = d[0]\n",
    "            tag_seq = d[1]\n",
    "            new2ori = None\n",
    "            d.append(new2ori)\n",
    "            d.append(word_seq)\n",
    "            d.append(self.seq_tag2id(tag_seq))\n",
    "            d.append(self.seq_intent2id(d[2]))\n",
    "            # here sep and cls will be added later\n",
    "            max_sen_len = max(max_sen_len, len(word_seq)+2)\n",
    "            # d = (tokens, tags, intents, da2triples(turn[\"dialog_act\"]), context(token id), new2ori, new_word_seq, tag2id_seq, intent2id_seq)\n",
    "            if data_key == \"train\":\n",
    "                for intent_id in d[-1]:\n",
    "                    self.intent_weight[intent_id] += 1\n",
    "        if data_key == \"train\":\n",
    "            train_size = len(self.data[\"train\"])\n",
    "            for intent, intent_id in self.intent2id.items():\n",
    "                neg_pos = (\n",
    "                    train_size - self.intent_weight[intent_id]\n",
    "                ) / self.intent_weight[intent_id]\n",
    "                self.intent_weight[intent_id] = np.log10(neg_pos)\n",
    "            self.intent_weight = torch.tensor(self.intent_weight)\n",
    "            self.max_context_len=max_context_len\n",
    "            self.max_sen_len=max_sen_len\n",
    "            print(\"max sen bert_policy len from train data\", self.max_sen_len)\n",
    "            print(\"max context bert_policy len from train data\", self.max_context_len)\n",
    "\n",
    "    def seq_tag2id(self, tags):\n",
    "        return [self.tag2id[x] for x in tags if x in self.tag2id]\n",
    "\n",
    "    def seq_id2tag(self, ids):\n",
    "        return [self.id2tag[x] for x in ids]\n",
    "\n",
    "    def seq_intent2id(self, intents):\n",
    "        return [self.intent2id[x] for x in intents if x in self.intent2id]\n",
    "\n",
    "    def seq_id2intent(self, ids):\n",
    "        return [self.id2intent[x] for x in ids]\n",
    "\n",
    "    def pad_batch(self, batch_data):\n",
    "        batch_size = len(batch_data)\n",
    "        max_sen_len = max([len(x[-3]) for x in batch_data]) + 2\n",
    "        word_mask_tensor = torch.zeros((batch_size, max_sen_len), dtype=torch.long)\n",
    "        word_seq_tensor = torch.zeros((batch_size, max_sen_len), dtype=torch.long)\n",
    "        tag_mask_tensor = torch.zeros((batch_size, max_sen_len), dtype=torch.long)\n",
    "        tag_seq_tensor = torch.zeros((batch_size, max_sen_len), dtype=torch.long)\n",
    "        intent_tensor = torch.zeros((batch_size, self.intent_dim), dtype=torch.float)\n",
    "        max_context_len = max([len(x[-5]) for x in batch_data])\n",
    "        context_mask_tensor = torch.zeros(\n",
    "            (batch_size, max_context_len), dtype=torch.long)\n",
    "        context_seq_tensor = torch.zeros(\n",
    "            (batch_size, max_context_len), dtype=torch.long)\n",
    "        for i in range(batch_size):\n",
    "            words = batch_data[i][-3]  #\n",
    "            tags = batch_data[i][-2]\n",
    "            intents = batch_data[i][-1]\n",
    "            words = [\"[CLS]\"] + words + [\"[SEP]\"]\n",
    "            indexed_tokens = self.tokenizer.convert_tokens_to_ids(words)\n",
    "            sen_len = len(words)\n",
    "            word_seq_tensor[i, :sen_len] = torch.LongTensor([indexed_tokens])\n",
    "            tag_seq_tensor[i, 1 : sen_len - 1] = torch.LongTensor(tags)\n",
    "            word_mask_tensor[i, :sen_len] = torch.LongTensor([1] * sen_len)\n",
    "            tag_mask_tensor[i, 1 : sen_len - 1] = torch.LongTensor([1] * (sen_len - 2))\n",
    "            for j in intents:\n",
    "                intent_tensor[i, j] = 1.0\n",
    "            context_len = len(batch_data[i][-5])\n",
    "            context_seq_tensor[i, :context_len] = torch.LongTensor([batch_data[i][-5]])\n",
    "            context_mask_tensor[i, :context_len] = torch.LongTensor([1] * context_len)\n",
    "\n",
    "        return word_seq_tensor,word_mask_tensor,tag_seq_tensor,tag_mask_tensor,intent_tensor,context_seq_tensor,context_mask_tensor\n",
    "    \n",
    "    def get_train_batch(self, batch_size):\n",
    "        batch_data = random.choices(self.data[\"train\"], k=batch_size)\n",
    "        return self.pad_batch(batch_data)\n",
    "\n",
    "    def yield_batches(self, batch_size, data_key):\n",
    "        batch_num = math.ceil(len(self.data[data_key]) / batch_size)\n",
    "        for i in range(batch_num):\n",
    "            batch_data = self.data[data_key][i * batch_size : (i + 1) * batch_size]\n",
    "            yield self.pad_batch(batch_data), batch_data, len(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointWithBert(nn.Module):\n",
    "    def __init__(self, model_config, slot_dim, intent_dim):\n",
    "        super(JointWithBert, self).__init__()\n",
    "        # count of intent and tag\n",
    "        self.slot_num_labels = slot_dim\n",
    "        self.intent_num_labels = intent_dim\n",
    "        # model\n",
    "        self.bert = BertModel.from_pretrained(model_config.pretrained_weights)\n",
    "        self.dropout = nn.Dropout(model_config.dropout)\n",
    "        self.hidden_units = model_config.hidden_units\n",
    "        \n",
    "        self.intent_classifier = nn.Linear(self.hidden_units, self.intent_num_labels)\n",
    "        self.slot_classifier = nn.Linear(self.hidden_units, self.slot_num_labels)\n",
    "        self.intent_hidden = nn.Linear(2 * self.bert.config.hidden_size, self.hidden_units)\n",
    "        self.slot_hidden = nn.Linear(2 * self.bert.config.hidden_size, self.hidden_units)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.intent_hidden.weight)\n",
    "        nn.init.xavier_uniform_(self.slot_hidden.weight)\n",
    "        nn.init.xavier_uniform_(self.intent_classifier.weight)\n",
    "        nn.init.xavier_uniform_(self.slot_classifier.weight)\n",
    "\n",
    "    def forward(self,word_seq_tensor,word_mask_tensor,context_seq_tensor,context_mask_tensor):\n",
    "        outputs = self.bert(input_ids=word_seq_tensor, attention_mask=word_mask_tensor)\n",
    "        # 获取每个token的output 输出[batch_size, seq_length, embedding_size] 如果做seq2seq 或者ner 用这个\n",
    "        sequence_output = outputs[0]\n",
    "        # 这个输出是获取句子的output\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        # 如果有上下文信息\n",
    "            # 将上下文信息进行bert训练并获得整个句子的output\n",
    "        context_output = self.bert(input_ids=context_seq_tensor, attention_mask=context_mask_tensor)[1]\n",
    "            # 将上下文得到输出和word_seq_tensor得到的输出进行拼接\n",
    "        sequence_output = torch.cat([context_output.unsqueeze(1).repeat(1, sequence_output.size(1), 1),sequence_output,],\n",
    "            dim=-1,)\n",
    "        # 将上下文得到输出和之前获取句子的output进行拼接\n",
    "        pooled_output = torch.cat([context_output, pooled_output], dim=-1)\n",
    "\n",
    "        # 经过dropout、Linear、relu层\n",
    "        sequence_output = nn.functional.relu(self.slot_hidden(self.dropout(sequence_output)))\n",
    "        pooled_output = nn.functional.relu(self.intent_hidden(self.dropout(pooled_output)))\n",
    "        # 经过dropout\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        # 经过Linear层\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "        outputs = (slot_logits,)\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "        outputs = outputs + (intent_logits,)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Config():\n",
    "    def __init__(self,):\n",
    "        self.pretrained_weights='./hfl/chinese-bert-wwm-ext'\n",
    "        self.train_data_path='./crosswoz_data/formated_train_nlu_data.json'\n",
    "        self.test_data_path='./crosswoz_data/formated_test_nlu_data.json'\n",
    "        self.dev_data_path='./crosswoz_data/formated_val_nlu_data.json'\n",
    "        self.hidden_units=1536\n",
    "        self.learning_rate=3.0e-5\n",
    "        self.bert_learning_rate=3e-5\n",
    "        self.other_learning_rate=3e-5\n",
    "        self.weight_decay=0.01\n",
    "        self.warmup_steps=0\n",
    "        self.save_weight_path='./crosswoz_data/output/saved_model/my-pytorch-joint-with-bert.pt'\n",
    "        self.save_model_path='./crosswoz_data/output/saved_model/my-pytorch-joint-with-bert.pth'\n",
    "        self.device='cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.eps=1e-8\n",
    "        self.batch_size=20\n",
    "        self.max_step=40000\n",
    "        self.check_step=1000\n",
    "        self.dropout=0.1\n",
    "        self.cut_sen_len=60\n",
    "        self.intent_vocab_path='./crosswoz_data/intent_vocab.json'\n",
    "        self.tag_vocab_path='./crosswoz_data/tag_vocab.json'\n",
    "        self.max_history=3\n",
    "        self.if_intent_weight=True\n",
    "        self.mask_loss=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_slot_da(da):\n",
    "    if da[0] in ['Inform','Recommend'] and '酒店设施' not in da[2]:\n",
    "        return True\n",
    "    return False\n",
    "def get_score(predict_golden):\n",
    "    TP,FP,FN=0,0,0\n",
    "    for item in predict_golden:\n",
    "        predicts=item['predict']\n",
    "        labels=item['golden']\n",
    "        for item in predicts:\n",
    "            if item in labels:\n",
    "                TP+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "        for item in labels:\n",
    "            if item not in predicts:\n",
    "                FN+=1\n",
    "    precision=1.0*TP/(TP+FP) if TP+FP else 0.0\n",
    "    recall=1.0*TP/(TP+FN) if TP+FN else 0.0\n",
    "    F1=2.0*precision*recall/(precision+recall) if precision+recall else 0.0\n",
    "    return precision,recall,F1\n",
    "def tag2das(word_seq,tag_seq):\n",
    "    assert len(word_seq)==len(tag_seq)\n",
    "    das=[]\n",
    "    i=0\n",
    "    while i<len(tag_seq):\n",
    "        tag=tag_seq[i]\n",
    "        if tag.startswith('B'):\n",
    "            intent,domain,slot=tag[2:].split('+')\n",
    "            value=word_seq[i]\n",
    "            j=i+1\n",
    "            while j<len(tag_seq):\n",
    "                if tag_seq[j].startswith('I') and tag_seq[j][2:]==tag[2:]:\n",
    "                    if word_seq[j].startswith('##'):\n",
    "                        value+=word_seq[j][2:]\n",
    "                    else:\n",
    "                        value+=word_seq[j]\n",
    "                    i+=1\n",
    "                    j+=1\n",
    "                else:\n",
    "                    break\n",
    "            das.append([intent,domain,slot,value])\n",
    "        i+=1\n",
    "    return das\n",
    "def recover_intent(dataloader,intent_logits,tag_logits,tag_mask_tensor,ori_word_seq,new2ori):\n",
    "    max_seq_len=tag_logits.size(0)\n",
    "    das=[]\n",
    "    for j in range(dataloader.intent_dim):\n",
    "        if intent_logits[j]>0:\n",
    "            intent,domain,slot,value=re.split('\\+',dataloader.id2intent[j])\n",
    "            das.append([intent,domain,slot,value])\n",
    "    tags=[]\n",
    "    for j in range(1,max_seq_len-1):\n",
    "        if tag_mask_tensor[j]==1:\n",
    "            value,tag_id=torch.max(tag_logits[j],dim=-1)\n",
    "            tags.append(dataloader.id2tag[tag_id.item()])\n",
    "    tag_intent=tag2das(ori_word_seq,tags)\n",
    "    das+=tag_intent\n",
    "    return das"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_loss_func(dataloader,config,intent_logits,intent_tensor,slot_logits,tag_seq_tensor,tag_mask_tensor,intent_loss_fct,slot_loss_fct):\n",
    "    if config.mask_loss:\n",
    "        active_tag_loss = tag_mask_tensor.view(-1) == 1\n",
    "            # I made some change for the view function\n",
    "        active_tag_logits = slot_logits.view(-1, slot_logits.size()[-1])[active_tag_loss]\n",
    "        active_tag_labels = tag_seq_tensor.view(-1)[active_tag_loss]\n",
    "    else:\n",
    "        active_tag_logits = slot_logits\n",
    "        active_tag_labels = tag_seq_tensor\n",
    "    slot_loss = slot_loss_fct(active_tag_logits, active_tag_labels)\n",
    "    intent_loss = intent_loss_fct(intent_logits, intent_tensor)\n",
    "    return slot_loss,intent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(config,model,dataloader,data_key,slot_loss_fct,intent_loss_fct):\n",
    "    model.eval()\n",
    "    val_slot_loss,val_intent_loss=0,0\n",
    "    predict_golden={'intent':[],'slot':[],'overall':[]}\n",
    "    score_result={'intent':[],'slot':[],'overall':[]}\n",
    "    for index,(model_inputs,batch_data,num_data) in tqdm(enumerate(dataloader.yield_batches(config.batch_size,data_key))):\n",
    "        model_inputs=tuple(item.to(config.device) for item in model_inputs)\n",
    "        word_seq_tensor,word_mask_tensor,tag_seq_tensor,tag_mask_tensor,intent_tensor,context_seq_tensor,context_mask_tensor=model_inputs\n",
    "        with torch.no_grad():\n",
    "            slot_logits,intent_logits=model.forward(word_seq_tensor,word_mask_tensor,context_seq_tensor,context_mask_tensor)\n",
    "\n",
    "            slot_loss,intent_loss=get_total_loss_func(dataloader,config,intent_logits,intent_tensor,slot_logits,tag_seq_tensor,tag_mask_tensor,intent_loss_fct,slot_loss_fct)\n",
    "\n",
    "        val_slot_loss+=slot_loss.item()*num_data\n",
    "        val_intent_loss+=intent_loss.item()*num_data\n",
    "        \n",
    "        for i in range(num_data):\n",
    "            predicts=recover_intent(dataloader,intent_logits[i],slot_logits[i],tag_mask_tensor[i],batch_data[i][0],batch_data[i][-4])\n",
    "            labels=batch_data[i][3]\n",
    "            predict_golden['overall'].append({'predict':predicts,'golden':labels})\n",
    "            predict_golden['intent'].append({'predict':[x for x in predicts if not is_slot_da(x)],'golden':[x for x in labels if not is_slot_da(x)]})\n",
    "            predict_golden['slot'].append({'predict':[x for x in predicts if is_slot_da(x)],'golden':[x for x in labels if is_slot_da(x)]})\n",
    "    for x in ['intent','slot','overall']:\n",
    "        precision,recall,F1=get_score(predict_golden[x])\n",
    "        score_result[x]=[precision,recall,F1]\n",
    "        print('-'*20+x+'-'*20)\n",
    "        print('Precision:{},Recall:{},F1:{}'.format(precision,recall,F1))\n",
    "    avg_slot_loss=val_slot_loss/len(dataloader.data[data_key])\n",
    "    avg_intent_loss=val_intent_loss/len(dataloader.data[data_key])\n",
    "    print('val_slot_loss:{}，val_intent_loss:{}'.format(avg_slot_loss,avg_intent_loss))\n",
    "    return avg_slot_loss,avg_intent_loss,score_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent_slot(utterance:str,context:list,config,dataloader,model):\n",
    "    # utterance: str, context: list\n",
    "    model.eval()\n",
    "\n",
    "    context_seq = dataloader.tokenizer.encode(\"[CLS] \" + \" [SEP] \".join(context[-config.max_history:]))\n",
    "    \n",
    "    \n",
    "    ori_word_seq=dataloader.tokenizer.tokenize(utterance)\n",
    "    ori_tag_seq = [\"O\"]*len(ori_word_seq)\n",
    "    \n",
    "    intents = []\n",
    "    da = []\n",
    "    word_seq,tag_seq,new2ori=ori_word_seq,ori_tag_seq,None\n",
    "\n",
    "    batch_data=[[ori_word_seq,ori_tag_seq,intents,da,context_seq,new2ori,word_seq,dataloader.seq_tag2id(tag_seq),dataloader.seq_intent2id(intents)]]\n",
    "    pad_batch=dataloader.pad_batch(batch_data)\n",
    "    pad_batch=tuple(t.to(config.device) for t in pad_batch)\n",
    "    \n",
    "    word_seq_tensor,word_mask_tensor,tag_seq_tensor,tag_mask_tensor,intent_tensor,context_seq_tensor,context_mask_tensor=pad_batch\n",
    "    with torch.no_grad():\n",
    "        slot_logits,intent_logits = model.forward(word_seq_tensor,word_mask_tensor,context_seq_tensor,context_mask_tensor)\n",
    "    das=recover_intent(dataloader,intent_logits[0],slot_logits[0],tag_mask_tensor[0],batch_data[0][0],batch_data[0][-4])\n",
    "    return das"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config,model,dataloader,slot_loss_fct,intent_loss_fct):\n",
    "    print(config.device)\n",
    "    bert_param_optimizer=list(model.bert.named_parameters())\n",
    "    bert_params=list(map(id,model.bert.parameters()))\n",
    "    other_param_optimizer=[(n,p) for n,p in model.named_parameters() if id(p) not in bert_params]\n",
    "    no_decay=['bias','LayerNorm.bias','LayerNorm.weight']\n",
    "    optimizer_grouped_parameters=[\n",
    "        {'params':[p for n,p in bert_param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay':config.weight_decay,'lr':config.bert_learning_rate},\n",
    "        {'params':[p for n,p in bert_param_optimizer if any(nd in n for nd in no_decay)],'weight_decay':0,'lr':config.bert_learning_rate},\n",
    "        {'params':[p for n,p in other_param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay':config.weight_decay,'lr':config.other_learning_rate},\n",
    "        {'params':[p for n,p in other_param_optimizer if any(nd in n for nd in no_decay)],'weight_decay':0,'lr':config.other_learning_rate}]\n",
    "    optimizer=AdamW(optimizer_grouped_parameters,lr=config.learning_rate,eps=config.eps)\n",
    "    scheduler=get_linear_schedule_with_warmup(optimizer,num_warmup_steps=config.warmup_steps,num_training_steps=config.max_step)\n",
    "    \n",
    "    train_slot_loss,train_intent_loss=0,0\n",
    "    best_dev_loss=float('inf')\n",
    "    total_train_samples=0\n",
    "    for step in tqdm(range(1,config.max_step+1)):\n",
    "        model.train()\n",
    "        batched_data=dataloader.get_train_batch(config.batch_size)\n",
    "        batched_data=tuple(item.to(config.device) for item in batched_data)\n",
    "        word_seq_tensor,word_mask_tensor,tag_seq_tensor,tag_mask_tensor,intent_tensor,context_seq_tensor,context_mask_tensor=batched_data\n",
    "\n",
    "        slot_logits,intent_logits=model.forward(word_seq_tensor,word_mask_tensor,context_seq_tensor,context_mask_tensor)\n",
    "\n",
    "        slot_loss,intent_loss=get_total_loss_func(dataloader,config,intent_logits,intent_tensor,slot_logits,tag_seq_tensor,tag_mask_tensor,intent_loss_fct,slot_loss_fct)\n",
    "        optimizer.zero_grad()\n",
    "        total_loss=slot_loss+intent_loss\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "        optimizer.step()\n",
    "        train_slot_loss+=slot_loss.item()*word_seq_tensor.size(0)\n",
    "        train_intent_loss+=intent_loss.item()*word_seq_tensor.size(0)\n",
    "        total_train_samples+=word_seq_tensor.size(0)\n",
    "        scheduler.step()\n",
    "        if step%config.check_step==0:\n",
    "            train_slot_loss=train_slot_loss/total_train_samples\n",
    "            train_intent_loss=train_intent_loss/total_train_samples\n",
    "            print('current_step{}/total_steps{},train_slot_loss:{},train_intent_loss:{}'.format(step,config.max_step,train_slot_loss,train_intent_loss))\n",
    "            avg_slot_loss,avg_intent_loss,score_result=evaluate(config,model,dataloader,'val',slot_loss_fct,intent_loss_fct)\n",
    "            avg_dev_loss=avg_slot_loss+avg_intent_loss\n",
    "            if avg_dev_loss<best_dev_loss:\n",
    "                best_dev_loss=avg_dev_loss\n",
    "                torch.save(model.state_dict(),config.save_weight_path)\n",
    "                print('model is saved to:{}'.format(config.save_weight_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain data:\\nmax sen bert_policy len 95\\nmax context bert_policy len 183\\ndev data:\\nmax sen bert_policy len 48\\nmax context bert_policy len 163\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train data:\n",
    "max sen bert_policy len 95\n",
    "max context bert_policy len 183\n",
    "dev data:\n",
    "max sen bert_policy len 48\n",
    "max context bert_policy len 163\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data has already been preprocessed before\n"
     ]
    }
   ],
   "source": [
    "config=Model_Config()\n",
    "pretrained_weights='./hfl/chinese-bert-wwm-ext'\n",
    "file_dir='./crosswoz_data'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "if not all([os.path.exists(config.train_data_path),\n",
    "            os.path.exists(config.test_data_path),\n",
    "            os.path.exists(config.dev_data_path),\n",
    "            os.path.exists(config.intent_vocab_path),\n",
    "            os.path.exists(config.tag_vocab_path)]):\n",
    "    preprocess_data(file_dir=file_dir,tokenizer=tokenizer,include_sys=True)\n",
    "    print('raw data was just successfully preprocessed')\n",
    "else:\n",
    "    print('raw data has already been preprocessed before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sen bert_policy len from train data 62\n",
      "max context bert_policy len from train data 267\n"
     ]
    }
   ],
   "source": [
    "dataloader=Dataloader(config.intent_vocab_path, config.tag_vocab_path, config.pretrained_weights, config.max_history)\n",
    "dataloader.load_data(config.train_data_path, \"train\", config.cut_sen_len)\n",
    "dataloader.load_data(config.dev_data_path, \"val\", config.cut_sen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model trained from exist base model\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(config.save_model_path):\n",
    "    model=torch.load(config.save_model_path,map_location=config.device)\n",
    "    print('model trained from exist base model')\n",
    "else:\n",
    "    model=JointWithBert(model_config=config, slot_dim=dataloader.tag_dim, intent_dim=dataloader.intent_dim)\n",
    "    if os.path.exists(config.save_weight_path):\n",
    "        model.load_state_dict(torch.load(config.save_weight_path))\n",
    "        print('model trained from exist base weight')\n",
    "    else:\n",
    "        print('model trained from scratch')\n",
    "if config.if_intent_weight:\n",
    "    intent_loss_fct = torch.nn.BCEWithLogitsLoss(pos_weight=dataloader.intent_weight.to(config.device))\n",
    "else:\n",
    "    intent_loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "slot_loss_fct = torch.nn.CrossEntropyLoss()\n",
    "model.to(config.device)\n",
    "evaluate_sign=False\n",
    "if os.path.exists(config.save_weight_path) or os.path.exists(config.save_model_path):\n",
    "    if evaluate_sign:\n",
    "        avg_slot_loss,avg_intent_loss,score_result=evaluate(config,model,dataloader,'val',slot_loss_fct,intent_loss_fct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n--------------------intent--------------------\\nPrecision:0.9606143552311436,Recall:0.9696085955487337,F1:0.9650905202047209\\n--------------------slot--------------------\\nPrecision:0.964492898579716,Recall:0.9147220641244546,F1:0.9389483933787731\\n--------------------overall--------------------\\nPrecision:0.9629540243755279,Recall:0.9356862285278771,F1:0.9491243198239719\\nval_slot_loss:0.06734158956858625，val_intent_loss:0.002130140893944007\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "--------------------intent--------------------\n",
    "Precision:0.9606143552311436,Recall:0.9696085955487337,F1:0.9650905202047209\n",
    "--------------------slot--------------------\n",
    "Precision:0.964492898579716,Recall:0.9147220641244546,F1:0.9389483933787731\n",
    "--------------------overall--------------------\n",
    "Precision:0.9629540243755279,Recall:0.9356862285278771,F1:0.9491243198239719\n",
    "val_slot_loss:0.06734158956858625，val_intent_loss:0.002130140893944007\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sign=False\n",
    "if train_sign:\n",
    "    train(config,model,dataloader,slot_loss_fct,intent_loss_fct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sign=False\n",
    "if not train_sign:\n",
    "    torch.save(model, config.save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Inform', '酒店', '价格', '805元'], ['Inform', '酒店', '评分', '4.7分']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance = \"价格比较贵805元，评分是4.7分。\"\n",
    "context = [\"你好，帮我推荐一个能提供24小时热水和洗衣服务的高档型酒店，谢谢。\",\"建议您去北京广电国际酒店。\",\"行啊，北京广电国际酒店的价格贵吗？评分是多少呢？\"] \n",
    "predict_intent_slot(utterance,context,config,dataloader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Request', '酒店', '价格', ''],\n",
       " ['Request', '酒店', '评分', ''],\n",
       " ['Inform', '酒店', '名称', '北京广电国际酒店']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance = \"\".join([\"行\",\"啊\",\"，\",\"北京\",\"广电\",\"国际\",\"酒店\",\"的\",\"价格\",\"贵\",\"吗\",\"？\",\"评分\",\"是\",\"多少\",\"呢\",\"？\"])\n",
    "context = [\"你好，帮我推荐一个能提供24小时热水和洗衣服务的高档型酒店，谢谢。\",\"建议您去北京广电国际酒店。\"]\n",
    "predict_intent_slot(utterance,context,config,dataloader,model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
