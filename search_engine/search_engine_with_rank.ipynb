{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file='dataset/news.csv'\n",
    "if os.path.exists(csv_file):\n",
    "    news=pd.read_csv(csv_file,encoding='gb18030',nrows=20000)\n",
    "    news['content']=news['content'].fillna('')\n",
    "    news['cut_words']=news['content'].apply(lambda x:' '.join(list(jieba.cut(x))))\n",
    "    news['cut_words'].to_csv('dataset/news_content.csv')\n",
    "    print('news csv has been successfully processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_and(vectors):\n",
    "    return reduce(lambda a,b:a&b,vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalEngine:\n",
    "    def __init__(self,corpus):\n",
    "        # token_pattern is set to be r\"(?u)\\b\\w\\w+\\b\" by default which can only accept words longer than two.\n",
    "        # token_pattern is set to be r\"(?u)\\b\\w+\\b\" which can accept single word or alpha.\n",
    "        # vocabulary can give words which will be used to build matrix\n",
    "        # max_df can filter words which have higher exist frequency in all docs\n",
    "        # tf is decided only by current doc, tf equals frequency in single doc.\n",
    "        # idf is decided by how many docs have this word and how many docs are given here.\n",
    "        # idf equals to 1+log((total_docs)/(docs_contain_thisword)) or 1+log((1+total_docs)/(1+docs_contain_thisword))\n",
    "        # tfidf means tf*idf.\n",
    "        self.vectorizer=TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",max_df=1.0,stop_words=[],vocabulary=None,use_idf=True,smooth_idf=True)\n",
    "        self.vectorizer.fit(corpus)\n",
    "        self.corpus=corpus\n",
    "        self.d2w=self.vectorizer.transform(corpus).toarray()\n",
    "        self.w2d=self.d2w.transpose()\n",
    "    def get_words_id(self,words):\n",
    "        ids=[self.vectorizer.vocabulary_[w] for w in words if w in self.vectorizer.vocabulary_]\n",
    "        return ids\n",
    "    def get_w2d_vectors(self,words):\n",
    "        vectors=self.w2d[self.get_words_id(words)]\n",
    "        return vectors\n",
    "    # get the idnexes of docs which have all the specific words\n",
    "    def get_combined_common_indices(self,words):\n",
    "        try:\n",
    "            indices=reduce_and([set(np.where(v)[0]) for v in self.get_w2d_vectors(words)])\n",
    "            return indices\n",
    "        except Exception as e:\n",
    "            return []\n",
    "    def get_sorted_indices(self,words):\n",
    "        indices=self.get_combined_common_indices(words)\n",
    "        query_vector=self.vectorizer.transform(words).toarray()[0]\n",
    "        sorted_indices=sorted(indices,key=lambda indice:cosine(query_vector,self.d2w[indice]),reverse=True)\n",
    "        return sorted_indices\n",
    "    def get_requested_text(self,words):\n",
    "        sorted_indices=self.get_sorted_indices(words)\n",
    "        output=[self.corpus[indice] for indice in sorted_indices]\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/stu_18701958249/.local/lib/python3.7/site-packages/jieba/__init__.py\", line 154, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmp7dbaowr7' -> '/tmp/jieba.cache'\n",
      "Loading model cost 0.880 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.63174505 0.        ]\n",
      " [0.37311881 0.37311881 0.27824521]\n",
      " [0.         0.         0.4711101 ]\n",
      " [0.         0.         0.4711101 ]\n",
      " [0.63174505 0.         0.        ]\n",
      " [0.         0.         0.4711101 ]\n",
      " [0.4804584  0.4804584  0.        ]\n",
      " [0.         0.4804584  0.35829137]\n",
      " [0.4804584  0.         0.35829137]]\n",
      "{'我': 4, '爱': 6, '吃': 1, '香蕉': 8, '你': 0, '苹果': 7, '没有': 5, '得': 3, '好': 2}\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "corpus=[\" \".join(list(jieba.cut(\"我爱吃香蕉\"))),\" \".join(list(jieba.cut(\"你爱吃苹果\"))),\" \".join(list(jieba.cut(\"苹果没有香蕉吃得好\")))]\n",
    "retrieval_engine=RetrievalEngine(corpus)\n",
    "print(retrieval_engine.w2d)\n",
    "print(retrieval_engine.vectorizer.vocabulary_)\n",
    "words=list(jieba.cut(\"喜欢水果\"))\n",
    "print(retrieval_engine.get_words_id(words))\n",
    "\n",
    "print(retrieval_engine.get_w2d_vectors(words))\n",
    "\n",
    "print(retrieval_engine.get_combined_common_indices(words))\n",
    "print(retrieval_engine.get_sorted_indices(words))\n",
    "print(retrieval_engine.get_requested_text(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
